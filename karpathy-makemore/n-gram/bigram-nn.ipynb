{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a18a06a-f769-463a-9477-3a461894f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8034d2a9-2eee-4259-b572-c37388c9be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cdbcad-9e85-4828-990c-3119b4ef8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55adbb02-2ba9-4700-8aa5-220be8f52129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create training set of bigrams (x,y) x is first char, y is next char\n",
    "xs, ys = [], [] # training data, inputs & targets (labels)\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426f2532-7453-4811-9e01-a3e333c32b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59b8b04-c5d4-42d5-b241-fd4b48fdfc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92d93d0-f31c-433a-9234-5f7f2a6c2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot plugin an integer index into a neuaral net input layer,\n",
    "# a common way is to use one-hot encoding\n",
    "# alternative is embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0cdc55-3d51-493c-a801-ed9f637c3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # 27 is vocab length\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b0216c-892d-4eac-9656-427456b3f6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here 5 is the number of all rows, each num is an example for NN\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3ceea8-34a4-448f-81ec-d3bbd7a8c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca700464-88db-4a43-8dd3-d41a68d145db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fff61c90d50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1583d5fc-faf4-4e86-a16e-0c4bd1e3f23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype # floats can feed into NNs, integers cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a12576f1-cf76-44e9-b7f6-bbe5f77103d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.6185,      0.1086,     -1.1273,     -1.0633,      0.5411,\n",
       "             -0.4729,      0.5715,      0.8517,      0.5346,     -0.4223,\n",
       "             -0.2711,     -0.2821,      1.3404,     -1.0604,      1.8775,\n",
       "             -1.2478,      1.2015,      0.2475,      0.1720,     -1.5493,\n",
       "             -0.0518,      0.9093,      1.1958,     -0.2353,     -0.7321,\n",
       "              0.4283,     -0.1908],\n",
       "        [     0.9048,     -0.0429,      0.1731,      0.6941,     -0.0184,\n",
       "              0.0519,      0.0723,      1.6095,      1.4492,     -1.5868,\n",
       "             -0.8741,     -0.4049,      1.1798,     -0.8427,      2.4752,\n",
       "             -1.4257,      0.2046,      0.5211,     -2.1498,      0.4892,\n",
       "             -3.0333,      1.9435,     -0.2906,      1.9726,     -2.1114,\n",
       "              0.4876,      1.5974],\n",
       "        [     2.4475,     -1.1140,     -0.4240,      0.9721,     -1.1362,\n",
       "              0.2693,     -0.2806,     -0.2900,      1.4563,     -0.3470,\n",
       "             -0.2047,     -1.6099,      1.0933,     -0.8259,     -0.1531,\n",
       "             -1.5039,      1.4559,     -0.4664,     -1.3140,      0.4196,\n",
       "              1.5301,      0.7245,      0.4296,     -2.5156,      1.3413,\n",
       "              0.7200,     -0.9957],\n",
       "        [    -0.7000,      0.7886,     -0.0295,      0.8451,      0.3005,\n",
       "              0.6974,      0.3255,      0.9339,     -0.7966,     -1.9418,\n",
       "             -0.1414,      2.1875,      0.4371,     -0.3437,     -1.4762,\n",
       "             -0.2882,      0.6901,      1.1119,     -1.4722,      0.9427,\n",
       "             -1.1923,     -1.1733,     -0.4593,      1.1038,     -1.9542,\n",
       "              0.1447,      1.1243],\n",
       "        [     0.1097,     -0.6171,      0.8160,      1.2947,      0.5614,\n",
       "             -0.2903,      1.6580,     -1.6841,     -0.6616,     -0.0631,\n",
       "             -0.6882,      0.4688,     -0.4650,     -1.6085,      0.7516,\n",
       "              0.9967,     -0.2330,      1.5819,     -1.4459,     -0.4215,\n",
       "              1.4615,      0.5255,     -0.1388,      0.1486,      1.1467,\n",
       "              0.1569,     -0.1566],\n",
       "        [     0.6037,     -0.4101,     -0.5221,     -0.2234,      0.5808,\n",
       "             -0.1093,     -1.2373,      0.4862,      0.2107,      0.3666,\n",
       "             -0.3545,     -0.7347,      1.6910,      0.9798,     -0.5625,\n",
       "             -0.4817,      1.8258,      0.6333,     -1.7044,      0.5344,\n",
       "             -0.8404,     -0.7661,      0.6371,     -0.3895,      0.5160,\n",
       "              0.6658,     -0.7926],\n",
       "        [     0.5192,     -1.2058,      0.3315,     -0.0040,     -0.1657,\n",
       "              0.1559,      1.9168,     -1.0455,     -1.0050,      0.0786,\n",
       "             -1.6060,      0.0976,      0.2387,     -2.2257,      0.2680,\n",
       "             -0.7419,      0.4668,      0.6136,     -0.2606,     -0.4500,\n",
       "             -1.0160,     -1.0922,     -0.5241,      1.6194,      2.0565,\n",
       "             -1.0207,      1.3668],\n",
       "        [     0.3150,     -0.4984,      2.7228,      0.1924,     -1.6989,\n",
       "             -0.4036,      1.0079,      1.4812,      0.4344,      1.0021,\n",
       "              0.8051,      0.2545,     -0.0560,     -0.7068,     -0.6220,\n",
       "             -1.2312,     -1.0957,     -0.2439,     -0.1355,      1.8317,\n",
       "             -0.5956,      0.2511,     -0.4336,     -0.0428,     -2.5682,\n",
       "             -0.7082,     -0.3421],\n",
       "        [    -1.2272,      0.6199,     -0.1482,     -0.3437,      0.1868,\n",
       "              0.0138,      0.2550,     -1.5491,      1.0692,     -0.0881,\n",
       "              0.3391,     -0.0874,      0.5967,      2.2527,      1.1180,\n",
       "              0.2482,     -0.6223,     -0.9316,     -0.6613,     -1.8807,\n",
       "              0.0519,      0.5039,     -0.5296,      0.0225,     -0.6201,\n",
       "              0.2013,     -1.2786],\n",
       "        [    -0.0624,     -0.8063,      0.6578,     -0.3786,      0.6105,\n",
       "             -0.3245,      1.0872,      0.1401,     -0.1376,      1.1479,\n",
       "              2.0962,      0.8166,      1.1960,      0.8264,     -0.7341,\n",
       "              1.7046,      1.8706,      1.8383,     -0.7915,      0.6120,\n",
       "              0.6736,      0.7970,      1.1756,      0.8588,     -1.3620,\n",
       "              0.2745,     -0.9212],\n",
       "        [     0.8691,      0.6731,     -0.6848,      1.0480,     -0.5029,\n",
       "              1.8815,     -1.5323,     -0.5022,      1.0426,      1.1338,\n",
       "             -0.4038,      1.3996,     -1.0064,     -0.6920,     -0.2485,\n",
       "             -1.7820,      0.5379,      0.0426,     -1.0145,     -1.4776,\n",
       "              0.8757,      0.5368,      0.0593,     -0.6567,     -1.7394,\n",
       "              0.2269,     -0.2653],\n",
       "        [     1.6572,     -0.6788,     -1.4314,     -1.0690,      0.1438,\n",
       "             -0.1241,     -1.2376,      0.8301,      0.2005,      0.6716,\n",
       "              0.0003,      1.1194,     -0.5130,      1.5573,      1.4626,\n",
       "             -0.7951,      1.2561,      0.5508,     -0.4299,      0.8029,\n",
       "             -0.3071,      1.5476,      0.0960,     -0.5817,     -0.8978,\n",
       "              0.9374,     -0.1553],\n",
       "        [    -0.3737,     -0.7275,     -0.6565,     -1.4198,     -0.5335,\n",
       "             -1.3131,      0.8339,     -0.9856,      0.6420,     -1.9518,\n",
       "              0.0805,      1.0351,      0.1366,     -1.5497,      1.8388,\n",
       "             -0.3721,     -0.4263,      1.8270,      1.2878,     -0.3688,\n",
       "             -1.0733,      0.4092,     -0.9414,      0.0248,     -0.5700,\n",
       "              0.7544,      1.3325],\n",
       "        [     0.1501,      1.3657,     -2.5121,     -1.1937,     -0.6937,\n",
       "              0.4379,      0.0884,     -0.1753,     -0.6446,     -0.5381,\n",
       "              0.2052,     -1.1520,      1.3022,      1.7582,     -0.1419,\n",
       "              0.4483,      0.8165,     -1.5402,      0.4907,      1.0945,\n",
       "             -0.8329,     -1.9147,     -0.6949,     -0.7988,     -1.0426,\n",
       "             -0.8250,     -0.5682],\n",
       "        [     0.3415,     -1.5261,     -0.5423,     -0.9662,     -1.2020,\n",
       "             -1.0989,     -0.2072,      1.4544,      0.4448,     -0.1652,\n",
       "              0.2468,     -0.5250,      1.5460,     -1.0630,     -0.5769,\n",
       "              1.3629,      1.8806,     -0.0810,     -0.7322,      0.8321,\n",
       "              0.3179,     -0.7439,     -0.3903,     -0.3022,      0.1465,\n",
       "              0.9655,     -1.0949],\n",
       "        [     0.7310,     -0.6748,     -1.0215,      0.1169,     -0.2061,\n",
       "              0.2439,     -0.8214,     -2.1353,     -1.1813,      0.7257,\n",
       "              1.4297,     -1.8193,      0.7556,     -0.4346,      0.8844,\n",
       "             -1.1437,     -0.6505,     -0.1243,     -1.4641,     -1.0512,\n",
       "             -0.1148,     -0.7953,      2.9967,     -0.0490,     -1.2038,\n",
       "              2.4175,     -0.6183],\n",
       "        [    -0.2554,     -0.1002,      0.1915,     -1.3091,     -0.4830,\n",
       "             -1.1166,      0.8070,      1.6265,      0.0416,     -0.7391,\n",
       "              1.3004,      0.1130,      0.4344,      1.4034,     -0.5547,\n",
       "             -0.4874,     -1.4674,     -0.8149,      0.6942,      1.6008,\n",
       "              1.0513,     -0.6290,     -0.5098,      1.1720,      0.0088,\n",
       "             -0.5087,      1.2846],\n",
       "        [    -0.6812,      1.0908,      0.9985,      0.2246,      1.5320,\n",
       "              0.5137,     -0.3007,      0.6117,      0.1124,     -0.5374,\n",
       "             -0.7516,     -0.9986,      0.4333,     -0.7119,     -0.0132,\n",
       "              0.9586,     -1.6626,      0.5388,      0.5683,      0.2171,\n",
       "              1.6030,     -0.3614,      1.4993,      0.8502,      1.4564,\n",
       "             -1.1318,     -0.2778],\n",
       "        [     2.0578,     -2.1125,     -0.4341,     -0.1679,      1.0959,\n",
       "              0.3272,      0.6290,      0.2051,     -0.3187,     -0.2985,\n",
       "             -0.4272,      1.4763,     -1.1202,      0.2280,     -0.1324,\n",
       "             -0.1878,      0.0854,      0.5523,      0.5335,      0.1627,\n",
       "              1.9986,     -2.3831,      0.1911,     -1.6569,      0.5405,\n",
       "             -0.5452,      1.0339],\n",
       "        [     0.1368,     -1.3605,     -0.8211,      0.2270,     -0.2547,\n",
       "              0.3885,     -0.9412,     -0.1200,     -0.1063,      0.1643,\n",
       "             -1.7714,     -0.0744,     -0.4770,     -0.4908,      1.7024,\n",
       "              1.3565,      0.5706,     -2.3162,     -1.1446,      0.6502,\n",
       "              1.5945,      0.0444,     -1.4761,      0.1042,     -2.1772,\n",
       "              0.4940,      1.2674],\n",
       "        [     0.3520,     -0.4485,     -0.5192,     -0.9954,      1.0872,\n",
       "              2.6430,      1.8060,     -0.8150,      0.3455,      0.7636,\n",
       "             -0.2040,      1.4172,      0.4123,      0.2749,     -1.1521,\n",
       "             -0.1382,      0.0688,      0.7226,      0.9533,      1.0136,\n",
       "              1.2345,      0.0343,     -0.4014,      1.8530,      0.3151,\n",
       "              0.0647,     -1.7744],\n",
       "        [    -1.1146,      1.2823,     -0.9227,      0.2002,      0.7561,\n",
       "              0.6490,      0.4921,     -0.2048,      0.0908,      0.7803,\n",
       "              0.1114,     -0.7238,      0.0267,     -1.8630,     -0.3687,\n",
       "             -0.2563,     -0.3201,      2.0112,     -1.1178,      0.0287,\n",
       "             -0.4620,      0.7589,      1.4026,     -0.9201,      1.1536,\n",
       "             -0.0703,      0.4038],\n",
       "        [    -0.9509,      0.5702,     -1.3519,     -1.3459,      0.2052,\n",
       "             -0.0169,      0.0552,     -0.2922,      0.0756,     -0.0482,\n",
       "             -1.1926,     -0.6092,      1.7979,      0.3062,      0.7193,\n",
       "             -1.2664,     -1.1866,     -0.1101,     -2.0194,      0.0372,\n",
       "              1.3413,      0.8100,     -0.7190,      1.2145,      1.4766,\n",
       "             -0.8873,     -0.0221],\n",
       "        [    -1.3264,      2.3294,      0.6531,     -0.0890,     -1.1133,\n",
       "              0.4848,     -0.0611,     -2.3731,     -0.2664,     -1.1940,\n",
       "              0.0767,     -0.3419,     -0.5721,      0.8811,     -0.6716,\n",
       "              2.0645,      0.3640,      0.0341,      0.7114,     -0.8086,\n",
       "             -0.0070,      0.7516,     -1.4147,     -1.6122,     -0.5372,\n",
       "              1.2943,      1.5829],\n",
       "        [     0.2860,      0.6567,      1.4353,      1.0170,     -0.4815,\n",
       "              1.0985,     -0.9201,      0.1597,      0.6722,      0.1682,\n",
       "             -1.7780,     -0.4843,     -1.0690,     -1.1232,      1.1075,\n",
       "              0.1806,     -1.0506,      0.1781,      0.3045,     -1.9216,\n",
       "             -0.2593,     -0.5751,     -1.3858,     -1.0203,      1.1033,\n",
       "              1.2004,     -1.9940],\n",
       "        [     0.9339,      0.0372,     -1.2012,      0.7730,      1.1271,\n",
       "             -0.3463,      1.3489,     -2.0010,      0.3327,     -0.4592,\n",
       "             -0.4639,      2.2388,      1.0575,      0.6471,     -1.7070,\n",
       "              0.3619,      0.6391,      0.4599,      1.6574,     -0.1333,\n",
       "             -1.8868,     -0.4522,      0.7220,      1.0778,     -0.7235,\n",
       "             -0.9089,      1.2776],\n",
       "        [    -0.5947,     -0.3239,     -0.5201,      0.2634,     -1.0765,\n",
       "              0.9254,      1.8079,      0.0188,     -0.9151,      0.5957,\n",
       "             -0.2854,     -0.7244,      0.3686,      0.7725,     -1.2890,\n",
       "             -0.7962,     -0.2454,      0.2057,     -0.6143,      0.9434,\n",
       "              0.1221,      0.8983,     -0.1571,     -0.5430,     -0.4922,\n",
       "              0.2126,     -0.7293]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights for a single input neuron,\n",
    "# they would get multiplied by the inputs\n",
    "# W = torch.randn((27,1)) # 1 means 1 single neuron\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27)) # last 27 means 27 neurons\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3de11b9-dad6-4aa2-9bb2-0c96128b1c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6185,  0.1086, -1.1273, -1.0633,  0.5411, -0.4729,  0.5715,  0.8517,\n",
       "          0.5346, -0.4223, -0.2711, -0.2821,  1.3404, -1.0604,  1.8775, -1.2478,\n",
       "          1.2015,  0.2475,  0.1720, -1.5493, -0.0518,  0.9093,  1.1958, -0.2353,\n",
       "         -0.7321,  0.4283, -0.1908],\n",
       "        [ 0.6037, -0.4101, -0.5221, -0.2234,  0.5808, -0.1093, -1.2373,  0.4862,\n",
       "          0.2107,  0.3666, -0.3545, -0.7347,  1.6910,  0.9798, -0.5625, -0.4817,\n",
       "          1.8258,  0.6333, -1.7044,  0.5344, -0.8404, -0.7661,  0.6371, -0.3895,\n",
       "          0.5160,  0.6658, -0.7926],\n",
       "        [ 0.1501,  1.3657, -2.5121, -1.1937, -0.6937,  0.4379,  0.0884, -0.1753,\n",
       "         -0.6446, -0.5381,  0.2052, -1.1520,  1.3022,  1.7582, -0.1419,  0.4483,\n",
       "          0.8165, -1.5402,  0.4907,  1.0945, -0.8329, -1.9147, -0.6949, -0.7988,\n",
       "         -1.0426, -0.8250, -0.5682],\n",
       "        [ 0.1501,  1.3657, -2.5121, -1.1937, -0.6937,  0.4379,  0.0884, -0.1753,\n",
       "         -0.6446, -0.5381,  0.2052, -1.1520,  1.3022,  1.7582, -0.1419,  0.4483,\n",
       "          0.8165, -1.5402,  0.4907,  1.0945, -0.8329, -1.9147, -0.6949, -0.7988,\n",
       "         -1.0426, -0.8250, -0.5682],\n",
       "        [ 0.9048, -0.0429,  0.1731,  0.6941, -0.0184,  0.0519,  0.0723,  1.6095,\n",
       "          1.4492, -1.5868, -0.8741, -0.4049,  1.1798, -0.8427,  2.4752, -1.4257,\n",
       "          0.2046,  0.5211, -2.1498,  0.4892, -3.0333,  1.9435, -0.2906,  1.9726,\n",
       "         -2.1114,  0.4876,  1.5974]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply one-hot encoded input with weights, matrix multiplication\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56fcbafe-e154-4deb-8465-ac8b579664e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7582)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3, 13 is the firing rate of 13th neuron on the 3rd input\n",
    "# this is achieved by the dot product betwen the input\n",
    "# and the 13th column of the W (weights) matrix\n",
    "#\n",
    "# Matrix multiplications allow to evaluate dot product (w1*i1 + w2*i2 + ...) in paralel\n",
    "(xenc @ W)[3,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3cf42db-03d9-414d-ad64-d59d5bdf2e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ede4916-ea0b-498a-a632-095dcb4141a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9796,  0.1475, -1.8490, -0.8161, -0.2210, -0.0869, -0.2783, -1.7039,\n",
       "         0.9756,  0.9659,  1.1439, -0.6201,  0.8191, -1.3979, -0.5276, -2.8809,\n",
       "         0.9757,  0.1932,  0.0943, -1.1195, -1.8835, -1.0973,  0.0888,  0.4025,\n",
       "         1.2505,  0.5417, -0.2581])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bdac1b5-338a-42cf-af85-48ddb04181fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.3979)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is W*x dot product\n",
    "(xenc[3] * W[:,13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bef4666-6522-4dae-ab28-536f6a212c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 27) @ (27, 1) matrix multiplication\n",
    "# 5 is number of total inputs, 1 is number of neurons\n",
    "# 27s would multiply and add\n",
    "# these are 5 activasions\n",
    "# We feed all (5) samples into the single neuron\n",
    "# This is the result of W*x+b but there's no b (bias)\n",
    "\n",
    "# (5,27) @ (27,27) -> (5,27)\n",
    "# 5 is the number of inputs, 27 (last in second matrix) is the number of neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f06e8a2-8865-4a2d-8a5a-6fe3c618cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we fed our 27 dimensional inputs into a\n",
    "# first layer of the neural net that has 27 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c53feda9-685d-4328-813d-e718665716a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0449, 0.0270, 0.0078, 0.0084, 0.0416, 0.0151, 0.0428, 0.0567, 0.0413,\n",
       "         0.0159, 0.0184, 0.0182, 0.0924, 0.0084, 0.1581, 0.0069, 0.0804, 0.0310,\n",
       "         0.0287, 0.0051, 0.0230, 0.0600, 0.0800, 0.0191, 0.0116, 0.0371, 0.0200],\n",
       "        [0.0467, 0.0170, 0.0152, 0.0204, 0.0457, 0.0229, 0.0074, 0.0415, 0.0315,\n",
       "         0.0369, 0.0179, 0.0123, 0.1386, 0.0681, 0.0146, 0.0158, 0.1586, 0.0481,\n",
       "         0.0046, 0.0436, 0.0110, 0.0119, 0.0483, 0.0173, 0.0428, 0.0497, 0.0116],\n",
       "        [0.0342, 0.1153, 0.0024, 0.0089, 0.0147, 0.0456, 0.0321, 0.0247, 0.0154,\n",
       "         0.0172, 0.0361, 0.0093, 0.1082, 0.1707, 0.0255, 0.0461, 0.0666, 0.0063,\n",
       "         0.0480, 0.0879, 0.0128, 0.0043, 0.0147, 0.0132, 0.0104, 0.0129, 0.0167],\n",
       "        [0.0342, 0.1153, 0.0024, 0.0089, 0.0147, 0.0456, 0.0321, 0.0247, 0.0154,\n",
       "         0.0172, 0.0361, 0.0093, 0.1082, 0.1707, 0.0255, 0.0461, 0.0666, 0.0063,\n",
       "         0.0480, 0.0879, 0.0128, 0.0043, 0.0147, 0.0132, 0.0104, 0.0129, 0.0167],\n",
       "        [0.0396, 0.0154, 0.0191, 0.0321, 0.0157, 0.0169, 0.0172, 0.0801, 0.0683,\n",
       "         0.0033, 0.0067, 0.0107, 0.0521, 0.0069, 0.1904, 0.0039, 0.0197, 0.0270,\n",
       "         0.0019, 0.0261, 0.0008, 0.1119, 0.0120, 0.1152, 0.0019, 0.0261, 0.0792]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log counts\n",
    "counts = logits.exp() # equivalent to N counts\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93781bb2-0615-4103-bafc-9805066020c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum() # a single row of probabilities should sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d9b991a-be3a-4b13-ac52-6171db846604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "609f8fc1-cb85-43dc-acd4-21fb52391a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for every of 5 input examples we have a row\n",
    "# that came out of a neural net\n",
    "\n",
    "# Agian, say for the first char '.' from the example\n",
    "# To feed '.' into a neural net:\n",
    "# 1. We got its index\n",
    "# 2. One hot encoded it\n",
    "# 3. Then it went into the neural net\n",
    "# 4. Probalility distributions came out as an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02de792d-2943-460e-8041-3f8a8c449861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0449, 0.0270, 0.0078, 0.0084, 0.0416, 0.0151, 0.0428, 0.0567, 0.0413,\n",
       "        0.0159, 0.0184, 0.0182, 0.0924, 0.0084, 0.1581, 0.0069, 0.0804, 0.0310,\n",
       "        0.0287, 0.0051, 0.0230, 0.0600, 0.0800, 0.0191, 0.0116, 0.0371, 0.0200])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are probabilities for '.' to come next\n",
    "probs[0] # [0] is '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40828f4f-2e51-486c-862c-e95b5b7bd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to optimize weights W to have good probabilities,\n",
    "# \"good\" is measured by the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf86312-0f16-40a7-9069-4c5cbe780504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY ------------------------------>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdb02add-c2f9-48ac-bfcb-23f8ca46592c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs # inputs to the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91ad76fd-fcfc-4396-b00f-5a695a9f03f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys # labels for the correct next char in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ca3168b-4fdf-4d8d-9a48-97b2d2b72d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bba8a236-0c78-4322-a3fe-3f2ad32f2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE FORWARD PASS!!!\n",
    "# all below are differentiable operations and we can backpropagate through\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax' layer in the NN\n",
    "# this is a normalization/activation function used to make NN output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44fe9735-4fac-47f8-a0a6-6adc1b2b6fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For every single example we have a vector of probabilities that sum to 1\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee13f576-1d0a-4148-ae1e-94aa881ae69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.012286250479519367\n",
      "log likelihood: -4.3992743492126465\n",
      "negative log likelihood: 4.3992743492126465\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050704151391983\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367684692144394\n",
      "log likelihood: -2.6080667972564697\n",
      "negative log likelihood: 2.6080667972564697\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.01497753243893385\n",
      "log likelihood: -4.2012038230896\n",
      "negative log likelihood: 4.2012038230896\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "# LOSS IS AN AVERAGE LOG LIKELIHOOD!!!\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bdea0db-d3f9-41c4-8eba-e91e1914baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can minimize the loss by tuning W (weights) by computing\n",
    "# the gradients of the loss with the respect to these W matrices\n",
    "# and so we did tune W to minimize the loss to find the good setting\n",
    "# for W using gradient based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c856a9f-ff70-41c8-8740-cf90a23aad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e24e81c9-46ad-478f-a772-d8833e602b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a84164c-beb7-4de1-840e-4cd47a14d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our NN is a single linear layer followed by softmax\n",
    "\n",
    "# Negative log likelihood is used for classification\n",
    "# Mean squared error is used for regression\n",
    "\n",
    "# Calculating loss:\n",
    "# looking at probs[0, 5] for the first example/row\n",
    "# looking at probs[1, 13] for the second example/row\n",
    "# looking at probs[2, 13] for the third example/row\n",
    "# looking at probs[3, 1] for the third example/row\n",
    "# looking at probs[4, 0] for the third example/row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e75bc6b4-58da-401a-8992-70b1c90471d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123),\n",
       " tensor(0.0181),\n",
       " tensor(0.0267),\n",
       " tensor(0.0737),\n",
       " tensor(0.0150))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are probabilities we actually need\n",
    "probs[0,5], probs[1,13], probs[2,13], probs[3,1], probs[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bceefc59-3cea-4bcd-bae5-454edd6b78a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4a625de-75f7-42eb-af96-6adafc0e71fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the torch way to get them\n",
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f759f69-a260-4e91-b7cb-f9bd948b5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ !!! OPTIMIZATION !!! (manual) --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f98b425-200a-4df4-8a58-773e68e60579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs # inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ddb5cee1-d74e-48ca-a64d-ed1a6528e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4dac9a0a-a960-49ce-9c5e-c5fcf115d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1954a105-5234-44e9-9af8-ba586111cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "89d57205-b152-483c-bd3c-538d7500cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6891887187957764\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a6996016-a17f-44ea-aaee-e66976fc8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # set gradients of weigts to 0\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "caf3e693-c1e2-4687-b44f-ce4382104577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23f8afbf-afcf-43fe-b8ca-575862e79d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0292],\n",
       "        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n",
       "          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n",
       "          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n",
       "          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0482,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every element of W.grad is telling us the influence\n",
    "# of that weight on the loss function\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "470439a9-c5de-48ca-a176-1520864beed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize weights where 0.1 is learning rate\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b5f99037-c2f1-42d4-85a9-ceccebef3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing forward --> backward --> optimize is called \"doing gradient descent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14427131-2f9b-4ea1-a36d-42ed4ef7fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- !!! OPTIMIZATION !!! (automatic) --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9d0d4a9d-b526-4b9c-a51f-d75b004b6059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "73c8499b-bba0-411f-844b-56e68c42d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4834256172180176\n",
      "2.4833855628967285\n",
      "2.4833483695983887\n",
      "2.4833130836486816\n",
      "2.4832799434661865\n",
      "2.4832475185394287\n",
      "2.4832165241241455\n",
      "2.483186960220337\n",
      "2.4831581115722656\n",
      "2.4831299781799316\n",
      "2.483103036880493\n",
      "2.483076333999634\n",
      "2.48305082321167\n",
      "2.4830257892608643\n",
      "2.483001232147217\n",
      "2.4829773902893066\n",
      "2.482954263687134\n",
      "2.48293137550354\n",
      "2.4829089641571045\n",
      "2.4828872680664062\n",
      "2.482866048812866\n",
      "2.482844829559326\n",
      "2.4828243255615234\n",
      "2.482804298400879\n",
      "2.4827847480773926\n",
      "2.482764959335327\n",
      "2.48274564743042\n",
      "2.48272705078125\n",
      "2.482708215713501\n",
      "2.4826903343200684\n",
      "2.4826724529266357\n",
      "2.4826550483703613\n",
      "2.482637405395508\n",
      "2.4826204776763916\n",
      "2.4826037883758545\n",
      "2.4825873374938965\n",
      "2.4825708866119385\n",
      "2.4825544357299805\n",
      "2.482538938522339\n",
      "2.482523202896118\n",
      "2.4825077056884766\n",
      "2.482492446899414\n",
      "2.4824774265289307\n",
      "2.4824626445770264\n",
      "2.482448101043701\n",
      "2.482433557510376\n",
      "2.48241925239563\n",
      "2.482405185699463\n",
      "2.482390880584717\n",
      "2.482377052307129\n",
      "2.48236346244812\n",
      "2.4823501110076904\n",
      "2.4823367595672607\n",
      "2.4823238849639893\n",
      "2.4823107719421387\n",
      "2.482297897338867\n",
      "2.4822850227355957\n",
      "2.4822723865509033\n",
      "2.482259750366211\n",
      "2.4822475910186768\n",
      "2.4822356700897217\n",
      "2.4822235107421875\n",
      "2.4822115898132324\n",
      "2.4821999073028564\n",
      "2.4821879863739014\n",
      "2.4821765422821045\n",
      "2.4821650981903076\n",
      "2.48215389251709\n",
      "2.482142686843872\n",
      "2.4821319580078125\n",
      "2.4821207523345947\n",
      "2.482109546661377\n",
      "2.4820988178253174\n",
      "2.482088327407837\n",
      "2.4820775985717773\n",
      "2.4820668697357178\n",
      "2.4820566177368164\n",
      "2.482046604156494\n",
      "2.4820363521575928\n",
      "2.4820261001586914\n",
      "2.482016086578369\n",
      "2.482006549835205\n",
      "2.481997013092041\n",
      "2.4819867610931396\n",
      "2.4819769859313965\n",
      "2.4819674491882324\n",
      "2.4819579124450684\n",
      "2.4819486141204834\n",
      "2.4819395542144775\n",
      "2.4819302558898926\n",
      "2.4819211959838867\n",
      "2.4819118976593018\n",
      "2.481903076171875\n",
      "2.4818942546844482\n",
      "2.4818851947784424\n",
      "2.4818766117095947\n",
      "2.481868028640747\n",
      "2.4818594455718994\n",
      "2.4818508625030518\n",
      "2.481842279434204\n"
     ]
    }
   ],
   "source": [
    "# gradient descent (training a net!)\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdim=True) # probabilities for next character\n",
    "  # 0.01*(W**2).mean() is a loss regularization\n",
    "  # 0.01 is regularization strength, it's like controlling N + how much in array mode\n",
    "  # It's a gravity force that pushes W to be 0\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85aaa6-92e6-4323-bcff-9513151ff4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W.exp() is essentially an N, but N was filled by counting\n",
    "# and W was initialized randomly and we let loss to guide us\n",
    "# to the exact same array\n",
    "\n",
    "# Later on with adding complexity to the neural net\n",
    "# Only forward pass would change, the rest, e.g. backpropagation\n",
    "# weights updates would remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5be9e5e6-ecd8-4acd-8d1d-475e02ffd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd7a6a-c612-4a6e-86ea-62cb4b2102c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
