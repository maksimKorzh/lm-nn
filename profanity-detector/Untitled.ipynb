{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb41286",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa18cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "max_len = 22           # what is the longest word in dataset to encode?\n",
    "batch_size = 64        # how many independent sequences will we process in parallel?\n",
    "n_embd = 128           # embedding size per character\n",
    "n_hidden = 64          # size of LSTM hidden state\n",
    "n_layer = 1            # number of LSTM layers\n",
    "max_iters = 5500       # total number of batches trained\n",
    "eval_iters = 200       # number of iterations to evaluate\n",
    "eval_interval = 100    # validation print interval\n",
    "learning_rate = 0.0001 # by how much weights update each iteration?\n",
    "\n",
    "# Same results across different platforms\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load data\n",
    "with open('input.txt') as f: lines = f.read().splitlines()\n",
    "words, labels = [], []\n",
    "for line in lines:\n",
    "    word, label = line.strip().split(',')\n",
    "    words.append(word.lower())\n",
    "    labels.append(int(label))\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "vocab_size = len(chars)+1\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# Encode a single word\n",
    "def encode_word(w):\n",
    "    encoded = [stoi.get(c, 0) for c in w]\n",
    "    if len(encoded) < max_len: encoded += [0] * (max_len - len(encoded))\n",
    "    else: encoded = encoded[:max_len]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37be04c",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37691aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, n_layer):\n",
    "      super().__init__()\n",
    "      self.embedding = nn.Embedding(vocab_size, n_embd, padding_idx=0)             # maps chars to vectors, ignores padding\n",
    "      self.lstm = nn.LSTM(n_embd, n_hidden, num_layers=n_layer, batch_first=True)  # processes char sequences\n",
    "      self.fc = nn.Linear(n_hidden, 1)                                             # outputs single logit\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "      x = self.embedding(x)\n",
    "      _, (hn, _) = self.lstm(x)\n",
    "      logits = self.fc(hn[-1]).squeeze(1)\n",
    "      loss = None\n",
    "      if targets is not None:\n",
    "          loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "      return logits, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
