{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de5ed5c",
   "metadata": {},
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc209b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload() # upload inputs.txt\n",
    "except: print('Running locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f28990",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4cae33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "max_len = 22            # what is the longest word in dataset to encode?\n",
    "batch_size = 64         # how many independent sequences will we process in parallel?\n",
    "n_embd = 128            # embedding size per character\n",
    "n_hidden = 64           # size of LSTM hidden state\n",
    "n_layer = 1             # number of LSTM layers\n",
    "max_iters = 100         # total number of batches trained\n",
    "eval_iters = 100        # number of iterations to evaluate\n",
    "eval_interval = 100     # validation print interval\n",
    "learning_rate = 0.0001  # by how much weights update each iteration?\n",
    "\n",
    "# Same results across different platforms\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load data\n",
    "with open('input.txt') as f: lines = f.read().splitlines()\n",
    "words, labels = [], []\n",
    "for line in lines:\n",
    "    word, label = line.strip().split(',')\n",
    "    words.append(word.lower())\n",
    "    labels.append(int(label))\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "vocab_size = len(chars)+1\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "# Encode a single word\n",
    "def encode_word(w):\n",
    "    encoded = [stoi.get(c, 0) for c in w]\n",
    "    if len(encoded) < max_len: encoded += [0] * (max_len - len(encoded))\n",
    "    else: encoded = encoded[:max_len]\n",
    "    return encoded\n",
    "\n",
    "# Encode data\n",
    "X = torch.tensor([encode_word(w) for w in words], dtype=torch.long)\n",
    "Y = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "# Split data into train/test subsets\n",
    "split_idx = int(0.9 * len(X))\n",
    "X_train, Y_train = X[:split_idx], Y[:split_idx]\n",
    "X_val,   Y_val   = X[split_idx:], Y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca603a",
   "metadata": {},
   "source": [
    "# Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b3b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f91784e",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7639453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, n_layer):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd, padding_idx=0)             # maps chars to vectors, ignores padding\n",
    "        self.lstm = nn.LSTM(n_embd, n_hidden, num_layers=n_layer, batch_first=True)  # processes char sequences\n",
    "        self.fc = nn.Linear(n_hidden, 1)                                             # outputs single logit\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embedding(x)\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        logits = self.fc(hn[-1]).squeeze(1)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "# Init model\n",
    "model = LSTM(vocab_size, n_embd, n_hidden, n_layer).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c19a5e",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88562dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data chunk loading\n",
    "def get_batch(split):\n",
    "    data_x = X_train if split == 'train' else X_val\n",
    "    data_y = Y_train if split == 'train' else Y_val\n",
    "    ix = torch.randint(len(data_x), (batch_size,))\n",
    "    x = data_x[ix].to(device)\n",
    "    y = data_y[ix].to(device)\n",
    "    return x, y\n",
    "\n",
    "# Evaluate model\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57df3f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08931f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'profanity_model.pt')\n",
    "\n",
    "# Download model from Google Colab\n",
    "try: files.download('profanity_model.pt')\n",
    "except: print('Running locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74dac8",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat counts\n",
    "bad_count = 0\n",
    "good_count = 0\n",
    "bad_correct = 0\n",
    "bad_wrong = 0\n",
    "good_correct = 0\n",
    "good_wrong = 0\n",
    "threshold = 0.5\n",
    "\n",
    "# Bulk test\n",
    "with open('input.txt') as f:\n",
    "    words = f.read().splitlines()\n",
    "    for w in words:\n",
    "        x = torch.tensor(encode_word(w), dtype=torch.long).unsqueeze(0).to(device) # (1, max_len)\n",
    "        with torch.no_grad():\n",
    "            logit, _ = model(x)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            if ',1' in w:\n",
    "                bad_count += 1\n",
    "                if prob > threshold: bad_correct += 1\n",
    "                if prob < threshold:\n",
    "                    #print(f'should be bad:\\t{prob:.4f}\\t{w.split(\",\")[0]}')\n",
    "                    bad_wrong += 1\n",
    "            if ',0' in w:\n",
    "                good_count += 1\n",
    "                if prob < threshold: good_correct += 1\n",
    "                if prob > threshold:\n",
    "                    #print(f'should be good:\\t{prob:.4f}\\t{w.split(\",\")[0]}')\n",
    "                    good_wrong += 1\n",
    "\n",
    "# Calculate results\n",
    "good_acc = good_correct / good_count * 100\n",
    "bad_acc = bad_correct / bad_count * 100\n",
    "overall_acc = (good_correct + bad_correct) / (good_count + bad_count) * 100\n",
    "print(f\"Bad accuracy: {bad_acc:.2f}%\")\n",
    "print(f\"Good accuracy: {good_acc:.2f}%\")\n",
    "print(f\"Overall accuracy: {overall_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Main loop\n",
    "print(\"Type a word to check if it's profanity. Type 'exit' to quit.\")\n",
    "while True:\n",
    "    word = input(\"Enter word: \").strip().lower()\n",
    "    if word.lower() == 'exit': break\n",
    "    x = torch.tensor(encode_word(word), dtype=torch.long).unsqueeze(0).to(device) # (1, max_len)\n",
    "    with torch.no_grad():\n",
    "        logit, _ = model(x)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "        print(prob)\n",
    "        print(\"Profanity\" if prob > 0.5 else \"Not profanity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
